# Introduction

## Basics

Masking - Means To hide something. Keeping most of the values as  zero and multiply (element wise) it with the features will ensure we can attend to the most important word as the the rest of the values will be zero. This will help in masking the unwated feature and next word predictions will become very strong. 

This is called `Selective Masking`

## Attention 

The process of selective masking is called `Attention` but to create these masks are not a straight-forward. 
In Transformers, the mask is generated via techniques which will be discussed shortly.


## Self Attention






# References
[Attention is all you need](https://)
