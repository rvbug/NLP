# LSTM

Due to the exploding/vanishing gradients in RNNs, LSTM was introduced which can remember longer sequences.

There are 3 gates, forget gate, input gate and output gate.
These gates will help us what to forget, switch to a new context, what to remember and what to pay attention to.
What is the data which it needs to pay attention to


Each gates have their own set of weights which will help them learn (yes they are fully diffentiable)

## gates and variables

<img width="903" alt="image" src="https://github.com/rvbug/NLP/assets/10928536/912354ef-5414-4945-81e7-cee09b18cd96">

















# References
[2019 LSTM Paper](https://arxiv.org/pdf/1909.09586.pdf)
[Chris Olah LSTM Introduction](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
[YT - StatQuest](https://youtu.be/YCzL96nL7j0)
[YT - MIT Lecture](https://youtu.be/ySEx_Bqxvvo)
[YT - Krish](https://www.youtube.com/watch?v=FLjn0H2bCvA)
